<!doctype html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang=""> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8" lang=""> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9" lang=""> <![endif]-->
<!--[if gt IE 8]><!-->
<html lang="">
    <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta name="description" content="Homepage of Dongqi Cai (蔡东琪)">
        <meta name="keywords" content="Dongqi Cai, 蔡东琪, Intel, BUPT, THU, computer vision, machine learning">
        <meta name="author" content="Dongqi Cai">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="google-site-verification" content="CpEOXAd4lZ-NOnH1yTtnlZt-4SWgVhUvcyvnmaoqMyE"/>
        <meta name="baidu-site-verification" content="WB6lqGSpq1"/>
        <title>Dongqi Cai (蔡东琪)</title>
        <link rel="stylesheet" href="css/bootstrap.min.css">
        <link rel="stylesheet" href="css/flexslider.css">
        <link rel="stylesheet" href="css/jquery.fancybox.css">
        <link rel="stylesheet" href="css/main.css">
        <link rel="stylesheet" href="css/responsive.css">
        <link rel="stylesheet" href="css/animate.min.css">
        <link rel="stylesheet" href="css/font-icon.css">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    </head>
	
    <body>
        <!-- header section -->
		<section class="banner" role="banner"> 
  <!--header navigation -->
  <header id="header">
    <div class="header-content clearfix">
      <nav class="navigation" role="navigation">
        <ul class="primary-nav">
          <li><a href="#intro">Introduction</a></li>
          <li><a href="#publications">Publications</a></li>
		  <li><a href="#awards">Awards</a></li>
          <li><a href="#intro">Contact</a></li>
        </ul>
      </nav>
      <a href="#" class="nav-toggle">Menu<span></span></a> </div>
  </header>
  <!--header navigation --> 
  <!-- banner text -->
  <div class="container">
    <div class="col-md-10 col-md-offset-1">
      <div class="banner-text text-center">
        <h1>Dongqi Cai</h1>
		<h1>蔡东琪</h1>
        <br><br>
        <nav role="navigation"> <a href="#intro" class="banner-btn"><img src="images/down-arrow.png" alt=""></a></nav>
      </div>
      <!-- banner text --> 
    </div>
  </div>
</section>
        <!-- header section -->         
        <!-- introduction section -->
        <section id="intro" class="section intro">
            <div class="container">
                <div class="row"> 
                    <!-- team member 1 -->
                    <div class="col-xs-6 col-md-4">
                        <div class="person"> <br>
                            <div class="person-content">
                                <img src="images/bighead.png" alt="" class="img-responsive" style="margin:auto">
                            </div>
                            <ul class="social-icons clearfix">
                                <li>
                                    <a href="https://www.linkedin.com/in/dongqi-cai-17707a28/" target="_blank"><span class="fa fa-linkedin"></span></a>
                                </li>
                                <li>
                                    <a href="mailto:caidonkey@gmail.com" title="E-mail:caidonkey@gmail.com"><span class="fa fa-envelope"></span></a>
                                </li>
                                <li>
                                    <a href="https://github.com/caidonkey" target="_blank"><span class="fa fa-github"></span></a>
                                </li>
                            </ul>
                        </div>
                    </div>
                    <div class="col-xs-12 col-md-8">
                    <h2>Short Bio</h2>
					<br>
                    <strong>Dongqi Cai</strong> is currently an AI research scientist at <a href="https://www.intel.com/content/www/us/en/research/overview.html">Intel Labs</a>. She got her Ph.D. degree from Multimedia Communication and Pattern Recognition Lab (<a href="">MCPRL</a>), Beijing University of Posts and Telecommunications (<a href="https://www.bupt.edu.cn/">BUPT</a>) in March 2016, supervised by <a href="https://teacher.bupt.edu.cn/sufei/en/jsxx/37002/jsxx/jsxx.htm">Prof. Fei Su</a>. She was a joint Postdoctoral Researcher of Intel and Tsinghua University (<a href="https://www.tsinghua.edu.cn/THU">THU</a>) during 2017 to 2019, advised by <a href="http://web.ee.tsinghua.edu.cn/zhangli/zh_CN/index.htm">Prof. Li Zhang</a>. &nbsp;
                    <div>
                    <br>
                    Her research interests include Computer Vision, Deep Learning and Machine Learning, specifically deep learning based visual recognition, multi-modality visual understanding and efficient AI application deployment.<br>
                    </div>
                    </div>
                </div>
            </div>
        </section>
        <!-- publications section -->
        <section id="publications" class="section teams">
            <div class="container">
                <div class="row">
                    <div class="container">
                        <div class="row text-center">
                            <h2 class="page-header">Publications</h2><br>
                        </div>
                        <!-- ICML 2023 -->
                        <div class="row">
                            <div class="col-xs-6 col-md-4">
                                <img src="images/publications/ske2grid.jpg" class="img-responsive">
                            </div>
                            <div class="col-xs-12 col-md-8">
                                <h4>Ske2Grid: Skeleton-to-Grid Representation Learning for Action Recognition&nbsp;</h4>
                                <h4><small>Intl. Conf. on Machine Learning (ICML), 2023</small></h4>
                                <h5 class="text-muted"><span class="text-primary">Dongqi Cai</span>, YangYuxuan Kang, <a href="https://yaoanbang.github.io/"><font color="#8a6d3b">Anbang Yao</a>, Yurong Chen</font></h5><br>
                                <br> 
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#abstract1" aria-controls="abstract1" data-toggle="tab">Abstract</a> 
                                    </li>
                                    <li role="presentation"><a href="https://openreview.net/pdf?id=SQtp4uUByd" target="_blank">Paper</a> 
                                    </li>
									<li role="presentation"><a href="https://github.com/OSVAI/Ske2Grid" target="_blank">Code</a> 
                                    </li>
                                    <li role="presentation"><a href="https://recorder-v3.slideslive.com/?share=82491&s=b931200c-7cc9-480a-b644-a6e35bd589fe" target="_blank">Video</a> 
                                    </li>
									<li role="presentation"><a href="#bibtex1" aria-controls="bibtex1" data-toggle="tab">Bibtex</a> 
                                    </li>
                                </ul>
                                <div class="tab-content"><br>
                                    <div role="tabpanel" class="tab-pane active" id="abstract1">
                                        <p style="text-transform:none">This paper presents Ske2Grid, a new representation learning framework for improved skeleton-based action recognition. In Ske2Grid, we define a regular convolution operation upon a novel grid representation of human skeleton, which is a compact image-like grid patch constructed and learned through three novel designs, namely graph-node index transform (GIT), up-sampling transform (UPT) and progressive learning strategy (PLS). We construct networks upon prevailing graph convolution networks and conduct experiments on six mainstream skeleton-based action recognition datasets. Experiments show that our Ske2Grid signiﬁcantly outperforms existing GCN-based solutions under different benchmark settings, without bells and whistles.</p>
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="bibtex1">
                                        <p style="text-transform:none">	@inproceedings{cai2023ske2grid,<br>
                			&nbsp;&nbsp;&nbsp;&nbsp;author = {Cai, Dongqi and Kang, Yangyuxuan and Yao, Anbang and Chen, Yurong},<br>
                			&nbsp;&nbsp;&nbsp;&nbsp;title = {Ske2Grid: Skeleton-to-Grid Representation Learning for Action Recognition},<br>
                			&nbsp;&nbsp;&nbsp;&nbsp;booktitle = {International Conference on Machine Learning},<br>
                			&nbsp;&nbsp;&nbsp;&nbsp;year={2023}<br>
							&nbsp;&nbsp;&nbsp;&nbsp;url={https://openreview.net/pdf?id=SQtp4uUByd}<br>
                    	}</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <hr>
						                        <!-- NeurIPS 2023 -->
                        <div class="row">
                            <div class="col-xs-6 col-md-4">
                                <img src="images/publications/DNR_p1.jpg" class="img-responsive">
                            </div>
                            <div class="col-xs-12 col-md-8">
                                <h4>Dynamic Normalization and Relay for Video Action Recognition&nbsp;</h4>
                                <h4><small>Advances in Neural Information Processing Systems (NeurIPS), 2021</small></h4>
                                <h5 class="text-muted"><span class="text-primary">Dongqi Cai</span>, <a href="https://yaoanbang.github.io/"><font color="#8a6d3b">Anbang Yao</a>, Yurong Chen</font></h5><br>
                                <br> 
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#abstract1" aria-controls="abstract1" data-toggle="tab">Abstract</a> 
                                    </li>
                                    <li role="presentation"><a href="https://proceedings.neurips.cc/paper/2021/file/5bd529d5b07b647a8863cf71e98d651a-Paper.pdf" target="_blank">Paper</a> 
                                    </li>
									<li role="presentation"><a href="https://github.com/caidonkey/dnr" target="_blank">Code</a> 
                                    </li>
                                    <li role="presentation"><a href="https://slideslive.com/38968652/dynamic-normalization-and-relay-for-video-action-recognition?ref=search-presentations-dynamic+normalization+and+relay" target="_blank">Video</a> 
                                    </li>
									<li role="presentation"><a href="#bibtex2" aria-controls="bibtex2" data-toggle="tab">Bibtex</a> 
                                    </li>
                                </ul>
                                <div class="tab-content"><br>
                                    <div role="tabpanel" class="tab-pane active" id="abstract1">
                                        <p style="text-transform:none">Convolutional Neural Networks (CNNs) have been the dominant model for video action recognition. Due to the huge memory and compute demand, popular action recognition networks need to be trained with small batch sizes, which makes learning discriminative spatial-temporal representations for videos become a challenging problem. In this paper, we present Dynamic Normalization and Relay (DNR), an improved normalization design, to augment the spatial-temporal representation learning of any deep action recognition model, adapting to small batch size training settings. DNR introduces two dynamic normalization relay modules to explore the potentials of cross-temporal and cross-layer feature distribution dependencies for estimating accurate layer-wise normalization parameters. These two DNR modules are instantiated as a light-weight recurrent structure conditioned on the current input features, and the normalization parameters estimated from the neighboring frames based features at the same layer or from the whole video clip based features at the preceding layers. Experimental results show that DNR brings large performance improvements to the baselines, achieving over 4.4% absolute margins in top-1 accuracy without training bells and whistles. More experiments on 3D backbones and several latest 2D spatial-temporal networks further validate its effectiveness. </p>
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="bibtex2">
                                        <p style="text-transform:none">	@inproceedings{cai2021dynamic,<br>
                			&nbsp;&nbsp;&nbsp;&nbsp;author = {Cai, Dongqi and Yao, Anbang and Chen, Yurong},<br>
                			&nbsp;&nbsp;&nbsp;&nbsp;title = {Dynamic Normalization and Relay for Video Action Recognition},<br>
                			&nbsp;&nbsp;&nbsp;&nbsp;booktitle = {Advances in Neural Information Processing Systems},<br>
                			&nbsp;&nbsp;&nbsp;&nbsp;year={2021}<br>
							&nbsp;&nbsp;&nbsp;&nbsp;url={https://proceedings.neurips.cc/paper/2021/file/5bd529d5b07b647a8863cf71e98d651a-Paper.pdf}<br>
                    	}</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <hr>
						                        <!-- Earlier Publications -->
                        <div class="row">
                            <div class="col-xs-6 col-md-4">
                                <img src="" class="img-responsive">
                            </div>
                            <div class="col-xs-12 col-md-8">
                                <h3>Earlier Publications</h3><br>
                            <p><li><strong>Learning visual knowledge memory networks for visual question answering</strong><br>
                                Zhou Su, Chen Zhu, Yinpeng Dong, <strong>Dongqi Cai</strong>, Yurong Chen and Jianguo Li<br>
                                In Proceedings of the IEEE conference on computer vision and pattern recognition (<strong>CVPR</strong>), 2018 
                                [<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Su_Learning_Visual_Knowledge_CVPR_2018_paper.pdf">Paper</a>]  
                            </li></p><br>
                            <p><li><strong>Learning supervised scoring ensemble for emotion recognition in the wild</strong><br>
                                Ping Hu, <strong>Dongqi Cai</strong>, Shandong Wang, Anbang Yao, and Yurong Chen<br>
                                In Proceedings of the 19th ACM International Conference on Multimodal Interaction (<strong>ICMI</strong>), 2017 
                                [<a href="https://dl.acm.org/doi/abs/10.1145/3136755.3143009">Paper</a>]  
                            </li></p><br>

                            <p><li><strong>HoloNet: towards robust emotion recognition in the wild</strong><br>
                                Anbang Yao, <strong>Dongqi Cai</strong>, Ping Hu, Shandong Wang, Liang Sha, and Yurong Chen<br>
                                In Proceedings of the 18th ACM international conference on multimodal interaction (<strong>ICMI</strong>), 2016 
                                [<a href="https://dl.acm.org/doi/10.1145/2993148.2997639">Paper</a>]  
                            </li></p><br>

                            <p><li><strong>Adaptive Synopsis of Non-Human Primates’ Surveillance Video Based on Behavior Classification</strong><br>
                                <strong>Dongqi Cai</strong>, Fei Su and Zhicheng Zhao<br>
                                In 22nd International Conference on MultiMedia Modeling (<strong>MMM</strong>), 2016 
                                [<a href="https://link.springer.com/chapter/10.1007/978-3-319-27671-7_60">Paper</a>]  
                            </li></p><br>

                            <p><li><strong>Deep CCA based super vector for action recognition</strong><br>
                                <strong>Dongqi Cai</strong> and Fei Su<br>
                                In IEEE International Conference on Image Processing (<strong>ICIP</strong>), 2015 
                                [<a href="https://ieeexplore.ieee.org/document/7351140">Paper</a>]  
                            </li></p><br>
							
							<p><li><strong>Local metric learning for EEG-based personal identification</strong><br>
                                <strong>Dongqi Cai</strong>, Kai Liu and Fei Su<br>
                                In IEEE International Conference on Acoustics, Speech and Signal Processing (<strong>ICASSP</strong>), 2015 
                                [<a href="https://ieeexplore.ieee.org/abstract/document/7178088">Paper</a>]  
                            </li></p><br>
							
							<p><li><strong>An adaptive symmetry detection algorithm based on local features</strong><br>
                                <strong>Dongqi Cai</strong>, Pengyu Li, Fei Su and Zhicheng Zhao<br>
                                In IEEE Visual Communications and Image Processing Conference (<strong>VCIP</strong>), 2014 
                                [<a href="https://ieeexplore.ieee.org/document/7051610">Paper</a>]  
                            </li></p><br>
							
							<p><li><strong>A joint NHP's behaviour classification method based on sticky HDP-HMM</strong><br>
                                <strong>Dongqi Cai</strong> and Fei Su<br>
                                In IEEE International Conference on Network Infrastructure and Digital Content (<strong>ICNIDC</strong>), 2014
                                [<a href="https://ieeexplore.ieee.org/document/7000296">Paper</a>]  
                            </li></p><br>
							
							<p><li><strong>HVS based visual quality assessment for digital cinema environment</strong><br>
                                <strong>Dongqi Cai</strong> and Fang Wei<br>
                                In IEEE InternationalConference on Network Infrastructure and Digital Content (<strong>ICNIDC</strong>), 2010 
                                [<a href="https://ieeexplore.ieee.org/document/5657784">Paper</a>]  
							</li></p><br>
                            </div>
                            </div>
                        <hr>
                </div>
            </div>
        </section>
        <!-- Award section -->
        <section id="awards" class="section testimonials">
            <div class="container">
                <div class="row">
                    <div class="container">
                        <div class="row text-center">
                            <h2 class="page-header">Awards</h2><br>
                        </div>
                        <!-- UAV -->
                        <div class="row">
                            <div class="col-xs-6 col-md-4">
                                <img src="" class="img-responsive">
                            </div>
                            <div class="col-xs-12 col-md-8">
							<p><li>2022 Intel Beijing RYC Top-3 Volunteers</li></p>
                            <p><li>Intel China Employee of the Year Award 2021</li></p>						
							<p><li>Intel China Award 2017, Highest Annual Team Award of Intel China</li></p>
							<p><li>Winner Team of EmotiW-AFEW 2017, out of 100+ Teams</li></p>
							<p><li>Gordy Award 2016(named after Intel’s co-founder Gordon Earle Moore), Highest Annual Research Award of Intel Labs</li></p>
							<p><li>1st Runner-up Team of EmotiW-AFEW 2016, out of ~100 Teams</li></p>	

                            </div>
                            </div>
                        </div>
                        <hr>


                        </div>
                    </div>
                </div>
            </div>
        </section>

			
			


<!-- Get a quote section -->
        <!-- Footer section -->
        <footer class="footer">
            <div class="footer-top section">
                <div class="container">
                    <div class="row">
                        <div class="footer-col col-xs-6 col-md-4">
                            <h5>&nbsp;&nbsp;&nbsp;&nbsp;&copy Dongqi Cai</h5>
                            <ul class="footer-share">
                                <li>
                                    <a href="https://scholar.google.com/citations?user=B77vH1AAAAAJ&hl=zh-CN" target="_blank"><i class="fa fa-google"></i></a>
                                </li>
                                <li>
                                    <a href="https://github.com/caidonkey" target="_blank"><i class="fa fa-github"></i></a>
                                </li>
                                <li>
                                    <a href="https://www.linkedin.com/in/dongqi-cai-17707a28/" target="_blank"><i class="fa fa-linkedin"></i></a>
                                </li>
                            </ul>
                        </div>
                        <div class="footer-col col-xs-12 col-md-8 pull-right">
                            <p class="pull-right"><a href="#"><br>Back to Top &nbsp;&nbsp;<i class="fa fa-chevron-circle-up"></i></a></p>
                        </div>
                    </div>
                </div>
            </div>
            <!-- footer top -->             
        </footer>
        <!-- Footer section -->         
        <!-- JS FILES -->         
        <script src="js/jquery.min.js"></script>
        <script src="js/bootstrap.min.js"></script>         
        <script src="js/jquery.flexslider-min.js"></script>         
        <script src="js/jquery.fancybox.pack.js"></script>         
        <script src="js/retina.min.js"></script>         
        <script src="js/modernizr.js"></script>         
        <script src="js/main.js"></script>
    </body>
</html>
